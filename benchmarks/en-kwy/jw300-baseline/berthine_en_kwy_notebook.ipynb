{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copie de starter_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/berthine/masakhane-mt/blob/master/benchmarks/en-kwy/jw300-baseline/berthine_en_kwy_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Igc5itf-xMGj"
      },
      "source": [
        "# Masakhane - Machine Translation for African Languages (Using JoeyNMT)\n",
        "\n",
        "### Languages: English-Kikongo\n",
        "\n",
        "### Author: Berthine Nyunga Mpinda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l929HimrxS0a"
      },
      "source": [
        "## Retrieve your data & make a parallel corpus\n",
        "\n",
        "If you are wanting to use the JW300 data referenced on the Masakhane website or in our GitHub repo, you can use `opus-tools` to convert the data into a convenient format. `opus_read` from that package provides a convenient tool for reading the native aligned XML files and to convert them to TMX format. The tool can also be used to fetch relevant files from OPUS on the fly and to filter the data as necessary. [Read the documentation](https://pypi.org/project/opustools-pkg/) for more details.\n",
        "\n",
        "Once you have your corpus files in TMX format (an xml structure which will include the sentences in your target language and your source language in a single file), we recommend reading them into a pandas dataframe. Thankfully, Jade wrote a silly `tmx2dataframe` package which converts your tmx file to a pandas dataframe. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oGRmDELn7Az0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "06a8959e-081a-4baf-8fd3-42cbbe43a31b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Cn3tgQLzUxwn",
        "colab": {}
      },
      "source": [
        "# TODO: Set your source and target languages. Keep in mind, these traditionally use language codes as found here:\n",
        "# These will also become the suffix's of all vocab and corpus files used throughout\n",
        "import os\n",
        "source_language = \"en\"\n",
        "target_language = \"kwy\" \n",
        "lc = False  # If True, lowercase the data.\n",
        "seed = 42  # Random seed for shuffling.\n",
        "tag = \"baseline\" # Give a unique name to your folder - this is to ensure you don't rewrite any models you've already submitted\n",
        "\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "os.environ[\"tag\"] = tag\n",
        "\n",
        "# This will save it to a folder in our gdrive instead!\n",
        "!mkdir -p \"/content/drive/My Drive/masakhane/$src-$tgt-$tag\"\n",
        "os.environ[\"gdrive_path\"] = \"/content/drive/My Drive/masakhane/%s-%s-%s\" % (source_language, target_language, tag)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kBSgJHEw7Nvx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a426aea5-aae7-407f-eec2-145e390e9ff2"
      },
      "source": [
        "!echo $gdrive_path"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masakhane/en-kwy-baseline\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gA75Fs9ys8Y9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "2842124e-247f-4158-bb84-3a43afd1a800"
      },
      "source": [
        "# Install opus-tools\n",
        "! pip install opustools-pkg"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting opustools-pkg\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/9f/e829a0cceccc603450cd18e1ff80807b6237a88d9a8df2c0bb320796e900/opustools_pkg-0.0.52-py3-none-any.whl (80kB)\n",
            "\r\u001b[K     |████                            | 10kB 27.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 20kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 30kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 40kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 51kB 3.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 61kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 71kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 3.7MB/s \n",
            "\u001b[?25hInstalling collected packages: opustools-pkg\n",
            "Successfully installed opustools-pkg-0.0.52\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xq-tDZVks7ZD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "fe2da991-81cf-4d1a-f6c1-cc0aded29ee9"
      },
      "source": [
        "# Downloading our corpus\n",
        "! opus_read -d JW300 -s $src -t $tgt -wm moses -w jw300.$src jw300.$tgt -q\n",
        "\n",
        "# extract the corpus file\n",
        "! gunzip JW300_latest_xml_$src-$tgt.xml.gz"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Alignment file /proj/nlpl/data/OPUS/JW300/latest/xml/en-kwy.xml.gz not found. The following files are available for downloading:\n",
            "\n",
            "   2 MB https://object.pouta.csc.fi/OPUS-JW300/v1/xml/en-kwy.xml.gz\n",
            " 263 MB https://object.pouta.csc.fi/OPUS-JW300/v1/xml/en.zip\n",
            "  17 MB https://object.pouta.csc.fi/OPUS-JW300/v1/xml/kwy.zip\n",
            "\n",
            " 282 MB Total size\n",
            "./JW300_latest_xml_en-kwy.xml.gz ... 100% of 2 MB\n",
            "./JW300_latest_xml_en.zip ... 100% of 263 MB\n",
            "./JW300_latest_xml_kwy.zip ... 100% of 17 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n48GDRnP8y2G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "outputId": "fb5d2418-a1ec-4f28-f7a1-16366c4ed2b7"
      },
      "source": [
        "# Download the global test set.\n",
        "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-any.en\n",
        "  \n",
        "# And the specific test set for this language pair.\n",
        "os.environ[\"trg\"] = target_language \n",
        "os.environ[\"src\"] = source_language \n",
        "\n",
        "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-$trg.en \n",
        "! mv test.en-$trg.en test.en\n",
        "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-$trg.$trg \n",
        "! mv test.en-$trg.$trg test.$trg"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-21 22:04:24--  https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-any.en\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 277791 (271K) [text/plain]\n",
            "Saving to: ‘test.en-any.en.1’\n",
            "\n",
            "\rtest.en-any.en.1      0%[                    ]       0  --.-KB/s               \rtest.en-any.en.1    100%[===================>] 271.28K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-08-21 22:04:24 (7.95 MB/s) - ‘test.en-any.en.1’ saved [277791/277791]\n",
            "\n",
            "--2020-08-21 22:04:25--  https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-kwy.en\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 205972 (201K) [text/plain]\n",
            "Saving to: ‘test.en-kwy.en’\n",
            "\n",
            "test.en-kwy.en      100%[===================>] 201.14K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2020-08-21 22:04:25 (8.82 MB/s) - ‘test.en-kwy.en’ saved [205972/205972]\n",
            "\n",
            "--2020-08-21 22:04:28--  https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-kwy.kwy\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 212465 (207K) [text/plain]\n",
            "Saving to: ‘test.en-kwy.kwy’\n",
            "\n",
            "test.en-kwy.kwy     100%[===================>] 207.49K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-08-21 22:04:28 (6.93 MB/s) - ‘test.en-kwy.kwy’ saved [212465/212465]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NqDG-CI28y2L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a3dc4371-0f0b-4438-972d-bad966ff65e1"
      },
      "source": [
        "# Read the test data to filter from train and dev splits.\n",
        "# Store english portion in set for quick filtering checks.\n",
        "en_test_sents = set()\n",
        "filter_test_sents = \"test.en-any.en\"\n",
        "j = 0\n",
        "with open(filter_test_sents) as f:\n",
        "  for line in f:\n",
        "    en_test_sents.add(line.strip())\n",
        "    j += 1\n",
        "print('Loaded {} global test sentences to filter from the training/dev data.'.format(j))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 3571 global test sentences to filter from the training/dev data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6D-_PqdXGJiB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "e6b24c4e-71fe-46f6-e723-d78f3d55eed6"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive\t   JW300_latest_xml_en-kwy.xml\tsample_data\ttest.en-any.en.1\n",
            "jw300.en   JW300_latest_xml_en.zip\ttest.en\t\ttest.kwy\n",
            "jw300.kwy  JW300_latest_xml_kwy.zip\ttest.en-any.en\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3CNdwLBCfSIl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "6970f226-36a7-4be2-dab5-51573c01f9e3"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# TMX file to dataframe\n",
        "source_file = 'jw300.' + source_language\n",
        "target_file = 'jw300.' + target_language\n",
        "\n",
        "source = []\n",
        "target = []\n",
        "skip_lines = []  # Collect the line numbers of the source portion to skip the same lines for the target portion.\n",
        "with open(source_file) as f:\n",
        "    for i, line in enumerate(f):\n",
        "        # Skip sentences that are contained in the test set.\n",
        "        if line.strip() not in en_test_sents:\n",
        "            source.append(line.strip())\n",
        "        else:\n",
        "            skip_lines.append(i)             \n",
        "with open(target_file) as f:\n",
        "    for j, line in enumerate(f):\n",
        "        # Only add to corpus if corresponding source was not skipped.\n",
        "        if j not in skip_lines:\n",
        "            target.append(line.strip())\n",
        "    \n",
        "print('Loaded data and skipped {}/{} lines since contained in test set.'.format(len(skip_lines), i))\n",
        "    \n",
        "df = pd.DataFrame(zip(source, target), columns=['source_sentence', 'target_sentence'])\n",
        "# if you get TypeError: data argument can't be an iterator is because of your zip version run this below\n",
        "#df = pd.DataFrame(list(zip(source, target)), columns=['source_sentence', 'target_sentence'])\n",
        "df.head(10)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded data and skipped 5241/193639 lines since contained in test set.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source_sentence</th>\n",
              "      <th>target_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Jehovah’s Word Is Alive</td>\n",
              "      <td>Diambu dia Yave Diamoyo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Highlights From the Book of Joshua</td>\n",
              "      <td>Nsasa za Nkand’a Yosua</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ENCAMPED on the Plains of Moab in 1473 B.C.E ....</td>\n",
              "      <td>VAVA Aneyisaele balwaka muna Ndimb’a Moabe mun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Their 40 - year wilderness sojourn is about to...</td>\n",
              "      <td>( Yosua 1 : ​ 11 ) E nkangalu wa ya 40 ma mvu ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A little over two decades later , the leader J...</td>\n",
              "      <td>Vioka makumole ma mvu , o Yosua wa mfidi a Isa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>And Jehovah your God was the one who kept push...</td>\n",
              "      <td>O Yave wa Nzambi eno , yandi okubakûla vovo nw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Written by Joshua in 1450 B.C.E . , the book o...</td>\n",
              "      <td>Muna Nkand’a Yosua muna ye tusansu twamfunu tw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>As we stand at the threshold of the promised n...</td>\n",
              "      <td>Ekolo tufinamang’o kota muna nz’ampa eyi yasil...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>With keen interest , then , let us give attent...</td>\n",
              "      <td>Yambula twabadika o nkand’a Yosua ye sungididi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>TO “ THE DESERT PLAINS OF JERICHO ”</td>\n",
              "      <td>MUNA “ NDIMB’A YERIKO ”</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     source_sentence                                    target_sentence\n",
              "0                            Jehovah’s Word Is Alive                            Diambu dia Yave Diamoyo\n",
              "1                 Highlights From the Book of Joshua                             Nsasa za Nkand’a Yosua\n",
              "2  ENCAMPED on the Plains of Moab in 1473 B.C.E ....  VAVA Aneyisaele balwaka muna Ndimb’a Moabe mun...\n",
              "3  Their 40 - year wilderness sojourn is about to...  ( Yosua 1 : ​ 11 ) E nkangalu wa ya 40 ma mvu ...\n",
              "4  A little over two decades later , the leader J...  Vioka makumole ma mvu , o Yosua wa mfidi a Isa...\n",
              "5  And Jehovah your God was the one who kept push...  O Yave wa Nzambi eno , yandi okubakûla vovo nw...\n",
              "6  Written by Joshua in 1450 B.C.E . , the book o...  Muna Nkand’a Yosua muna ye tusansu twamfunu tw...\n",
              "7  As we stand at the threshold of the promised n...  Ekolo tufinamang’o kota muna nz’ampa eyi yasil...\n",
              "8  With keen interest , then , let us give attent...  Yambula twabadika o nkand’a Yosua ye sungididi...\n",
              "9                TO “ THE DESERT PLAINS OF JERICHO ”                            MUNA “ NDIMB’A YERIKO ”"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hvgFJhMy6178",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        },
        "outputId": "c3e19b1c-7527-495f-bcd6-bd8291c44565"
      },
      "source": [
        "df.source_sentence[100]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Later , Joshua assembles all the tribes of Israel at Shechem .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "luzCebRY7FP0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        },
        "outputId": "6ec1143c-51af-4b7b-d838-f1bf641f15ae"
      },
      "source": [
        "df.target_sentence[100]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Kuna kwalanda , Yosua walunganesa makanda mawonso ma Isaele kuna Sekeme .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TM6Uy1-W7Pxw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9163f56e-45a3-4336-cd6c-5c508453ea7d"
      },
      "source": [
        "len(df)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "188399"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YkuK3B4p2AkN"
      },
      "source": [
        "## Pre-processing and export\n",
        "\n",
        "It is generally a good idea to remove duplicate translations and conflicting translations from the corpus. In practice, these public corpora include some number of these that need to be cleaned.\n",
        "\n",
        "In addition we will split our data into dev/test/train and export to the filesystem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M_2ouEOH1_1q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "14a862ad-6a42-4447-c0f2-5688bbd8b0dc"
      },
      "source": [
        "# drop duplicate translations\n",
        "df_pp = df.drop_duplicates()\n",
        "\n",
        "# drop conflicting translations\n",
        "# (this is optional and something that you might want to comment out \n",
        "# depending on the size of your corpus)\n",
        "df_pp.drop_duplicates(subset='source_sentence', inplace=True)\n",
        "df_pp.drop_duplicates(subset='target_sentence', inplace=True)\n",
        "\n",
        "# Shuffle the data to remove bias in dev set selection.\n",
        "df_pp = df_pp.sample(frac=1, random_state=seed).reset_index(drop=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z_1BwAApEtMk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "22d85353-1ddb-488a-ee27-fb24bdf351ce"
      },
      "source": [
        "# Install fuzzy wuzzy to remove \"almost duplicate\" sentences in the\n",
        "# test and training sets.\n",
        "! pip install fuzzywuzzy\n",
        "! pip install python-Levenshtein\n",
        "import time\n",
        "from fuzzywuzzy import process\n",
        "import numpy as np\n",
        "\n",
        "# reset the index of the training set after previous filtering\n",
        "df_pp.reset_index(drop=False, inplace=True)\n",
        "\n",
        "# Remove samples from the training data set if they \"almost overlap\" with the\n",
        "# samples in the test set.\n",
        "\n",
        "# Filtering function. Adjust pad to narrow down the candidate matches to\n",
        "# within a certain length of characters of the given sample.\n",
        "def fuzzfilter(sample, candidates, pad):\n",
        "  candidates = [x for x in candidates if len(x) <= len(sample)+pad and len(x) >= len(sample)-pad] \n",
        "  if len(candidates) > 0:\n",
        "    return process.extractOne(sample, candidates)[1]\n",
        "  else:\n",
        "    return np.nan\n",
        "\n",
        "# NOTE - This might run slow depending on the size of your training set. We are\n",
        "# printing some information to help you track how long it would take. \n",
        "scores = []\n",
        "start_time = time.time()\n",
        "for idx, row in df_pp.iterrows():\n",
        "  scores.append(fuzzfilter(row['source_sentence'], list(en_test_sents), 5))\n",
        "  if idx % 1000 == 0:\n",
        "    hours, rem = divmod(time.time() - start_time, 3600)\n",
        "    minutes, seconds = divmod(rem, 60)\n",
        "    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds), \"%0.2f percent complete\" % (100.0*float(idx)/float(len(df_pp))))\n",
        "\n",
        "# Filter out \"almost overlapping samples\"\n",
        "df_pp['scores'] = scores\n",
        "df_pp = df_pp[df_pp['scores'] < 95]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading https://files.pythonhosted.org/packages/43/ff/74f23998ad2f93b945c0309f825be92e04e0348e062026998b5eefef4c33/fuzzywuzzy-0.18.0-py2.py3-none-any.whl\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n",
            "Collecting python-Levenshtein\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-Levenshtein) (49.2.0)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.0-cp36-cp36m-linux_x86_64.whl size=144790 sha256=54841b37c0df095414201796e47e1a6787f22663d14f5687593e75c4b56b1381\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n",
            "Successfully built python-Levenshtein\n",
            "Installing collected packages: python-Levenshtein\n",
            "Successfully installed python-Levenshtein-0.12.0\n",
            "00:00:00.08 0.00 percent complete\n",
            "00:00:20.49 0.59 percent complete\n",
            "00:00:40.73 1.17 percent complete\n",
            "00:01:01.32 1.76 percent complete\n",
            "00:01:21.77 2.34 percent complete\n",
            "00:01:41.78 2.93 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '⇩']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00:02:01.56 3.51 percent complete\n",
            "00:02:21.92 4.10 percent complete\n",
            "00:02:41.41 4.68 percent complete\n",
            "00:03:01.43 5.27 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '․ ․']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00:03:21.46 5.85 percent complete\n",
            "00:03:41.19 6.44 percent complete\n",
            "00:04:00.57 7.02 percent complete\n",
            "00:04:20.59 7.61 percent complete\n",
            "00:04:40.21 8.20 percent complete\n",
            "00:04:59.48 8.78 percent complete\n",
            "00:05:19.21 9.37 percent complete\n",
            "00:05:38.30 9.95 percent complete\n",
            "00:05:58.23 10.54 percent complete\n",
            "00:06:18.26 11.12 percent complete\n",
            "00:06:37.99 11.71 percent complete\n",
            "00:06:57.42 12.29 percent complete\n",
            "00:07:18.36 12.88 percent complete\n",
            "00:07:39.67 13.46 percent complete\n",
            "00:07:59.71 14.05 percent complete\n",
            "00:08:20.63 14.63 percent complete\n",
            "00:08:41.54 15.22 percent complete\n",
            "00:09:02.20 15.80 percent complete\n",
            "00:09:22.06 16.39 percent complete\n",
            "00:09:41.58 16.98 percent complete\n",
            "00:10:01.17 17.56 percent complete\n",
            "00:10:21.29 18.15 percent complete\n",
            "00:10:41.53 18.73 percent complete\n",
            "00:11:00.82 19.32 percent complete\n",
            "00:11:20.11 19.90 percent complete\n",
            "00:11:40.22 20.49 percent complete\n",
            "00:11:59.61 21.07 percent complete\n",
            "00:12:19.86 21.66 percent complete\n",
            "00:12:39.44 22.24 percent complete\n",
            "00:12:59.89 22.83 percent complete\n",
            "00:13:19.01 23.41 percent complete\n",
            "00:13:38.66 24.00 percent complete\n",
            "00:13:58.70 24.59 percent complete\n",
            "00:14:19.66 25.17 percent complete\n",
            "00:14:39.86 25.76 percent complete\n",
            "00:14:59.76 26.34 percent complete\n",
            "00:15:18.83 26.93 percent complete\n",
            "00:15:37.95 27.51 percent complete\n",
            "00:15:57.55 28.10 percent complete\n",
            "00:16:16.69 28.68 percent complete\n",
            "00:16:36.36 29.27 percent complete\n",
            "00:16:56.56 29.85 percent complete\n",
            "00:17:15.43 30.44 percent complete\n",
            "00:17:34.23 31.02 percent complete\n",
            "00:17:53.09 31.61 percent complete\n",
            "00:18:12.26 32.20 percent complete\n",
            "00:18:31.62 32.78 percent complete\n",
            "00:18:50.11 33.37 percent complete\n",
            "00:19:08.84 33.95 percent complete\n",
            "00:19:28.10 34.54 percent complete\n",
            "00:19:47.04 35.12 percent complete\n",
            "00:20:06.10 35.71 percent complete\n",
            "00:20:25.24 36.29 percent complete\n",
            "00:20:44.07 36.88 percent complete\n",
            "00:21:03.49 37.46 percent complete\n",
            "00:21:22.56 38.05 percent complete\n",
            "00:21:42.16 38.63 percent complete\n",
            "00:22:00.96 39.22 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '․ ․ ․ ․']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00:22:20.06 39.80 percent complete\n",
            "00:22:39.12 40.39 percent complete\n",
            "00:22:58.58 40.98 percent complete\n",
            "00:23:17.32 41.56 percent complete\n",
            "00:23:36.22 42.15 percent complete\n",
            "00:23:55.32 42.73 percent complete\n",
            "00:24:14.48 43.32 percent complete\n",
            "00:24:34.92 43.90 percent complete\n",
            "00:24:53.98 44.49 percent complete\n",
            "00:25:12.97 45.07 percent complete\n",
            "00:25:32.18 45.66 percent complete\n",
            "00:25:51.33 46.24 percent complete\n",
            "00:26:10.43 46.83 percent complete\n",
            "00:26:29.59 47.41 percent complete\n",
            "00:26:48.17 48.00 percent complete\n",
            "00:27:07.52 48.59 percent complete\n",
            "00:27:26.26 49.17 percent complete\n",
            "00:27:45.93 49.76 percent complete\n",
            "00:28:05.41 50.34 percent complete\n",
            "00:28:24.76 50.93 percent complete\n",
            "00:28:43.82 51.51 percent complete\n",
            "00:29:02.57 52.10 percent complete\n",
            "00:29:21.22 52.68 percent complete\n",
            "00:29:40.22 53.27 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00:29:58.68 53.85 percent complete\n",
            "00:30:17.41 54.44 percent complete\n",
            "00:30:35.85 55.02 percent complete\n",
            "00:30:55.21 55.61 percent complete\n",
            "00:31:14.05 56.20 percent complete\n",
            "00:31:32.63 56.78 percent complete\n",
            "00:31:52.43 57.37 percent complete\n",
            "00:32:12.73 57.95 percent complete\n",
            "00:32:31.64 58.54 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '*']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00:32:51.10 59.12 percent complete\n",
            "00:33:10.29 59.71 percent complete\n",
            "00:33:30.01 60.29 percent complete\n",
            "00:33:49.12 60.88 percent complete\n",
            "00:34:08.24 61.46 percent complete\n",
            "00:34:27.75 62.05 percent complete\n",
            "00:34:46.72 62.63 percent complete\n",
            "00:35:05.47 63.22 percent complete\n",
            "00:35:24.15 63.81 percent complete\n",
            "00:35:43.35 64.39 percent complete\n",
            "00:36:01.80 64.98 percent complete\n",
            "00:36:20.44 65.56 percent complete\n",
            "00:36:39.58 66.15 percent complete\n",
            "00:36:58.51 66.73 percent complete\n",
            "00:37:17.57 67.32 percent complete\n",
            "00:37:36.50 67.90 percent complete\n",
            "00:37:55.05 68.49 percent complete\n",
            "00:38:14.48 69.07 percent complete\n",
            "00:38:33.89 69.66 percent complete\n",
            "00:38:52.79 70.24 percent complete\n",
            "00:39:12.12 70.83 percent complete\n",
            "00:39:31.53 71.41 percent complete\n",
            "00:39:50.77 72.00 percent complete\n",
            "00:40:10.05 72.59 percent complete\n",
            "00:40:28.90 73.17 percent complete\n",
            "00:40:48.32 73.76 percent complete\n",
            "00:41:07.98 74.34 percent complete\n",
            "00:41:26.74 74.93 percent complete\n",
            "00:41:45.79 75.51 percent complete\n",
            "00:42:04.44 76.10 percent complete\n",
            "00:42:24.18 76.68 percent complete\n",
            "00:42:43.90 77.27 percent complete\n",
            "00:43:02.55 77.85 percent complete\n",
            "00:43:21.46 78.44 percent complete\n",
            "00:43:40.14 79.02 percent complete\n",
            "00:43:59.41 79.61 percent complete\n",
            "00:44:18.10 80.20 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '․ ․ ․ ․ ․']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00:44:37.21 80.78 percent complete\n",
            "00:44:56.17 81.37 percent complete\n",
            "00:45:15.94 81.95 percent complete\n",
            "00:45:34.75 82.54 percent complete\n",
            "00:45:53.98 83.12 percent complete\n",
            "00:46:12.95 83.71 percent complete\n",
            "00:46:32.22 84.29 percent complete\n",
            "00:46:50.86 84.88 percent complete\n",
            "00:47:09.74 85.46 percent complete\n",
            "00:47:28.64 86.05 percent complete\n",
            "00:47:47.89 86.63 percent complete\n",
            "00:48:06.81 87.22 percent complete\n",
            "00:48:25.71 87.81 percent complete\n",
            "00:48:44.70 88.39 percent complete\n",
            "00:49:03.77 88.98 percent complete\n",
            "00:49:22.73 89.56 percent complete\n",
            "00:49:41.98 90.15 percent complete\n",
            "00:50:01.01 90.73 percent complete\n",
            "00:50:20.61 91.32 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '․ ․ ․ ․ ․ ․']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00:50:39.88 91.90 percent complete\n",
            "00:50:59.30 92.49 percent complete\n",
            "00:51:18.41 93.07 percent complete\n",
            "00:51:37.07 93.66 percent complete\n",
            "00:51:56.12 94.24 percent complete\n",
            "00:52:14.40 94.83 percent complete\n",
            "00:52:33.25 95.41 percent complete\n",
            "00:52:52.47 96.00 percent complete\n",
            "00:53:11.72 96.59 percent complete\n",
            "00:53:30.50 97.17 percent complete\n",
            "00:53:49.68 97.76 percent complete\n",
            "00:54:08.34 98.34 percent complete\n",
            "00:54:27.74 98.93 percent complete\n",
            "00:54:46.69 99.51 percent complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hxxBOCA-xXhy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "outputId": "9c362b06-80f1-4381-e056-c9c1bee1c8c7"
      },
      "source": [
        "# This section does the split between train/dev for the parallel corpora then saves them as separate files\n",
        "# We use 1000 dev test and the given test set.\n",
        "import csv\n",
        "\n",
        "# Do the split between dev/train and create parallel corpora\n",
        "num_dev_patterns = 1000\n",
        "\n",
        "# Optional: lower case the corpora - this will make it easier to generalize, but without proper casing.\n",
        "if lc:  # Julia: making lowercasing optional\n",
        "    df_pp[\"source_sentence\"] = df_pp[\"source_sentence\"].str.lower()\n",
        "    df_pp[\"target_sentence\"] = df_pp[\"target_sentence\"].str.lower()\n",
        "\n",
        "# Julia: test sets are already generated\n",
        "dev = df_pp.tail(num_dev_patterns) # Herman: Error in original\n",
        "stripped = df_pp.drop(df_pp.tail(num_dev_patterns).index)\n",
        "\n",
        "with open(\"train.\"+source_language, \"w\") as src_file, open(\"train.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in stripped.iterrows():\n",
        "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
        "    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
        "    \n",
        "with open(\"dev.\"+source_language, \"w\") as src_file, open(\"dev.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in dev.iterrows():\n",
        "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
        "    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
        "\n",
        "#stripped[[\"source_sentence\"]].to_csv(\"train.\"+source_language, header=False, index=False)  # Herman: Added `header=False` everywhere\n",
        "#stripped[[\"target_sentence\"]].to_csv(\"train.\"+target_language, header=False, index=False)  # Julia: Problematic handling of quotation marks.\n",
        "\n",
        "#dev[[\"source_sentence\"]].to_csv(\"dev.\"+source_language, header=False, index=False)\n",
        "#dev[[\"target_sentence\"]].to_csv(\"dev.\"+target_language, header=False, index=False)\n",
        "\n",
        "# Doublecheck the format below. There should be no extra quotation marks or weird characters.\n",
        "! head train.*\n",
        "! head dev.*"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> train.en <==\n",
            "* However , the family study is not the only time to teach children .\n",
            "However , reaching out for privileges in the congregation does mean striving to meet the qualifications set out in the Scriptures .\n",
            "How did Adam use his free will in a good way ?\n",
            "Should such situations cause you to conclude that it was a mistake to start on the journey and that you should abandon the automobile ?\n",
            "From studying the Bible , I came to understand that I have a responsibility to the Giver of life , Jehovah .\n",
            "Many people in Tuva live in remote communities that are hard to reach with the Kingdom message .\n",
            "Why might the Israelites have been fearful at seeing the glory of God that Moses reflected ?\n",
            "Although we ‘ hate every false path , ’ we may need to ask God to act in our behalf so that we do not succumb to some temptation to break his law .\n",
            "God’s holy spirit was poured out upon them , the apostle Peter gave a stirring talk explaining the meaning of this miracle , and some 3,000 became believers and were baptized .\n",
            "Then the voice said : “ Stop calling defiled the things God has cleansed . ”\n",
            "\n",
            "==> train.kwy <==\n",
            "* Kansi , e ntangw’elongi dia esi nzo ke yau kaka ko i ntangwa amase bafwete longa o wana .\n",
            "Kansi , kele vo ozolele tambula kiyekwa muna nkutakani kafwete lungisa oma melombwanga muna Nkand’a Nzambi .\n",
            "Aweyi Adami kasadila nswa wa kuyisolela mu mpila yambote ?\n",
            "Nga mambu mama mafwete kufila mu yindula vo dia uzowa wavanga mu yantika e nkangalu wau ? Nga obembola e kalu ?\n",
            "Mun’elongi diame dia Nkand’a Nzambi , yabakula vo mbebe ngina yau kwa Yave wa Mvani a moyo .\n",
            "Kuna nsi ya Tuva wantu ayingi mu zunga yandá bezingilanga , diampasi dikalanga mu wá nsangu za Kintinu .\n",
            "Ekuma Aneyisaele bamwena wonga vava bamona Mose wamwesanga nkembo a Nzambi ?\n",
            "( Tini kia 124 , 125 ) Kana una vo ‘ tusaulanga konso ngyend’a luvunu , ’ tufwete lombanga kwa Nzambi katusadisa kimana twalembi kulula e nsiku miandi .\n",
            "Mwand’avelela wa Nzambi wabayitalela , i bosi o Petelo wa ntumwa wasonga e nsasa y’esivi diadi kwa nkangu . Mazunda matatu ma wantu bakwikila yo vubwa .\n",
            "Muna nkumbu ntatu miami , e ndinga yamvovesanga vo : “ Yambula yikila e lekwa ina Nzambi kavelelese vo yafunzuka . ”\n",
            "==> dev.en <==\n",
            "Gone will be the need for hospitals and medications .\n",
            "3 : 27 .\n",
            "Such a circumstance can be very distressing .\n",
            "If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "Saved From God’s Wrath\n",
            "I feared that people would stop noticing me and see only a wheelchair with a sickly woman .\n",
            "A fine example is one of the best teachers .\n",
            "It is a pleasure and a precious privilege to share that Bible - based hope with others , is it not ?\n",
            "‘ You don’t think about doing something contrary to Jehovah’s law , ’ he said .\n",
            "In the Bethel family there , I was thrilled to be surrounded by many spiritually mature older brothers and sisters .\n",
            "\n",
            "==> dev.kwy <==\n",
            "Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "3 : ​ 27 .\n",
            "Ediadi dilenda kikilu kutukendeleka .\n",
            "Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "Tulenda Vuluka Muna Lumbu kia Makasi ma Nzambi\n",
            "Wonga yakala wau , kadi yayindulanga vo wantu ke badi kunsia diaka sungididi ko yo mona kaka nkento wayela muna kalu dia mvanguki .\n",
            "O songa mbandu ambote i mpila isundidi balenda kubalongela .\n",
            "I lau diampwena dia samuna e vuvu kiaki kia Nkand’a Nzambi kw’akaka .\n",
            "Wavova vo : ‘ Kuyindula vanga diambu ko dilenda kulula nsiku a Yave . ’\n",
            "Kuna Betele yakalanga entwadi ye mpangi zayingi z’anunu azikuka muna mwanda .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "epeCydmCyS8X"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Installation of JoeyNMT\n",
        "\n",
        "JoeyNMT is a simple, minimalist NMT package which is useful for learning and teaching. Check out the documentation for JoeyNMT [here](https://joeynmt.readthedocs.io)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iBRMm4kMxZ8L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "60b42662-0d25-49a2-ec00-ddd7dc21fb9c"
      },
      "source": [
        "# Install JoeyNMT\n",
        "! git clone https://github.com/joeynmt/joeynmt.git\n",
        "! cd joeynmt; pip3 install ."
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'joeynmt'...\n",
            "remote: Enumerating objects: 2479, done.\u001b[K\n",
            "Receiving objects:   0% (1/2479)   \rReceiving objects:   1% (25/2479)   \rReceiving objects:   2% (50/2479)   \rReceiving objects:   3% (75/2479)   \rReceiving objects:   4% (100/2479)   \rReceiving objects:   5% (124/2479)   \rReceiving objects:   6% (149/2479)   \rReceiving objects:   7% (174/2479)   \rReceiving objects:   8% (199/2479)   \rReceiving objects:   9% (224/2479)   \rReceiving objects:  10% (248/2479)   \rReceiving objects:  11% (273/2479)   \rReceiving objects:  12% (298/2479)   \rReceiving objects:  13% (323/2479)   \rReceiving objects:  14% (348/2479)   \rReceiving objects:  15% (372/2479)   \rReceiving objects:  16% (397/2479)   \rReceiving objects:  17% (422/2479)   \rReceiving objects:  18% (447/2479)   \rReceiving objects:  19% (472/2479)   \rReceiving objects:  20% (496/2479)   \rReceiving objects:  21% (521/2479)   \rReceiving objects:  22% (546/2479)   \rReceiving objects:  23% (571/2479)   \rReceiving objects:  24% (595/2479)   \rReceiving objects:  25% (620/2479)   \rReceiving objects:  26% (645/2479)   \rReceiving objects:  27% (670/2479)   \rReceiving objects:  28% (695/2479)   \rReceiving objects:  29% (719/2479)   \rReceiving objects:  30% (744/2479)   \rReceiving objects:  31% (769/2479)   \rReceiving objects:  32% (794/2479)   \rReceiving objects:  33% (819/2479)   \rReceiving objects:  34% (843/2479)   \rReceiving objects:  35% (868/2479)   \rReceiving objects:  36% (893/2479)   \rReceiving objects:  37% (918/2479)   \rReceiving objects:  38% (943/2479)   \rReceiving objects:  39% (967/2479)   \rReceiving objects:  40% (992/2479)   \rReceiving objects:  41% (1017/2479)   \rReceiving objects:  42% (1042/2479)   \rReceiving objects:  43% (1066/2479)   \rReceiving objects:  44% (1091/2479)   \rReceiving objects:  45% (1116/2479)   \rReceiving objects:  46% (1141/2479)   \rReceiving objects:  47% (1166/2479)   \rReceiving objects:  48% (1190/2479)   \rReceiving objects:  49% (1215/2479)   \rReceiving objects:  50% (1240/2479)   \rReceiving objects:  51% (1265/2479)   \rReceiving objects:  52% (1290/2479)   \rReceiving objects:  53% (1314/2479)   \rReceiving objects:  54% (1339/2479)   \rReceiving objects:  55% (1364/2479)   \rReceiving objects:  56% (1389/2479)   \rReceiving objects:  57% (1414/2479)   \rReceiving objects:  58% (1438/2479)   \rReceiving objects:  59% (1463/2479)   \rReceiving objects:  60% (1488/2479)   \rReceiving objects:  61% (1513/2479)   \rReceiving objects:  62% (1537/2479)   \rReceiving objects:  63% (1562/2479)   \rReceiving objects:  64% (1587/2479)   \rReceiving objects:  65% (1612/2479)   \rReceiving objects:  66% (1637/2479)   \rReceiving objects:  67% (1661/2479)   \rReceiving objects:  68% (1686/2479)   \rReceiving objects:  69% (1711/2479)   \rReceiving objects:  70% (1736/2479)   \rReceiving objects:  71% (1761/2479)   \rReceiving objects:  72% (1785/2479)   \rReceiving objects:  73% (1810/2479)   \rReceiving objects:  74% (1835/2479)   \rReceiving objects:  75% (1860/2479)   \rReceiving objects:  76% (1885/2479)   \rReceiving objects:  77% (1909/2479)   \rReceiving objects:  78% (1934/2479)   \rReceiving objects:  79% (1959/2479)   \rReceiving objects:  80% (1984/2479)   \rReceiving objects:  81% (2008/2479)   \rReceiving objects:  82% (2033/2479)   \rReceiving objects:  83% (2058/2479)   \rReceiving objects:  84% (2083/2479)   \rReceiving objects:  85% (2108/2479)   \rReceiving objects:  86% (2132/2479)   \rReceiving objects:  87% (2157/2479)   \rReceiving objects:  88% (2182/2479)   \rremote: Total 2479 (delta 0), reused 0 (delta 0), pack-reused 2479\u001b[K\n",
            "Receiving objects:  89% (2207/2479)   \rReceiving objects:  90% (2232/2479)   \rReceiving objects:  91% (2256/2479)   \rReceiving objects:  92% (2281/2479)   \rReceiving objects:  93% (2306/2479)   \rReceiving objects:  94% (2331/2479)   \rReceiving objects:  95% (2356/2479)   \rReceiving objects:  96% (2380/2479)   \rReceiving objects:  97% (2405/2479)   \rReceiving objects:  98% (2430/2479)   \rReceiving objects:  99% (2455/2479)   \rReceiving objects: 100% (2479/2479)   \rReceiving objects: 100% (2479/2479), 2.65 MiB | 30.11 MiB/s, done.\n",
            "Resolving deltas:   0% (0/1732)   \rResolving deltas:   1% (18/1732)   \rResolving deltas:   4% (85/1732)   \rResolving deltas:   5% (91/1732)   \rResolving deltas:   7% (124/1732)   \rResolving deltas:   8% (141/1732)   \rResolving deltas:   9% (157/1732)   \rResolving deltas:  10% (175/1732)   \rResolving deltas:  11% (192/1732)   \rResolving deltas:  12% (215/1732)   \rResolving deltas:  14% (244/1732)   \rResolving deltas:  15% (264/1732)   \rResolving deltas:  16% (278/1732)   \rResolving deltas:  17% (295/1732)   \rResolving deltas:  18% (312/1732)   \rResolving deltas:  19% (336/1732)   \rResolving deltas:  20% (347/1732)   \rResolving deltas:  21% (364/1732)   \rResolving deltas:  22% (385/1732)   \rResolving deltas:  23% (401/1732)   \rResolving deltas:  24% (425/1732)   \rResolving deltas:  27% (470/1732)   \rResolving deltas:  30% (533/1732)   \rResolving deltas:  31% (540/1732)   \rResolving deltas:  35% (607/1732)   \rResolving deltas:  37% (649/1732)   \rResolving deltas:  39% (690/1732)   \rResolving deltas:  40% (699/1732)   \rResolving deltas:  45% (791/1732)   \rResolving deltas:  67% (1169/1732)   \rResolving deltas:  72% (1258/1732)   \rResolving deltas:  73% (1272/1732)   \rResolving deltas:  76% (1325/1732)   \rResolving deltas:  78% (1351/1732)   \rResolving deltas:  81% (1410/1732)   \rResolving deltas:  82% (1423/1732)   \rResolving deltas:  83% (1440/1732)   \rResolving deltas:  84% (1455/1732)   \rResolving deltas:  85% (1475/1732)   \rResolving deltas:  86% (1492/1732)   \rResolving deltas:  87% (1508/1732)   \rResolving deltas:  88% (1530/1732)   \rResolving deltas:  89% (1547/1732)   \rResolving deltas:  90% (1565/1732)   \rResolving deltas:  96% (1671/1732)   \rResolving deltas:  98% (1699/1732)   \rResolving deltas:  99% (1715/1732)   \rResolving deltas: 100% (1732/1732)   \rResolving deltas: 100% (1732/1732), done.\n",
            "Processing /content/joeynmt\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.16.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (7.0.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.18.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (49.2.0)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.6.0+cu101)\n",
            "Requirement already satisfied: tensorflow>=1.14 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (2.3.0)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.3.1)\n",
            "Collecting sacrebleu>=1.3.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/d3/be980ad7cda7c4bbfa97ee3de062fb3014fc1a34d6dd5b82d7b92f8d6522/sacrebleu-1.4.13-py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.6MB/s \n",
            "\u001b[?25hCollecting subword-nmt\n",
            "  Downloading https://files.pythonhosted.org/packages/74/60/6600a7bc09e7ab38bc53a48a20d8cae49b837f93f5842a41fe513a694912/subword_nmt-0.3.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (3.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.10.1)\n",
            "Collecting pyyaml>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 13.3MB/s \n",
            "\u001b[?25hCollecting pylint\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/13/519c1264a134beab2be4bac8dd3e64948980a5ca7833b31cf0255b21f20a/pylint-2.6.0-py3-none-any.whl (325kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 53.3MB/s \n",
            "\u001b[?25hCollecting six==1.12\n",
            "  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
            "Collecting wrapt==1.11.1\n",
            "  Downloading https://files.pythonhosted.org/packages/67/b2/0f71ca90b0ade7fad27e3d20327c996c6252a2ffe88f50a95bba7434eda9/wrapt-1.11.1.tar.gz\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.9.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (3.12.4)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.34.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.2.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.3.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.31.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (2.3.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (3.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext->joeynmt==0.0.1) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext->joeynmt==0.0.1) (2.23.0)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (0.10.0)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from seaborn->joeynmt==0.0.1) (1.0.5)\n",
            "Collecting isort<6,>=4.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b5/9b/f9e9307c89a80552f298cef17a62fa856b3f5220436338886d5eab64d4fa/isort-5.4.2-py3-none-any.whl (94kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 13.5MB/s \n",
            "\u001b[?25hCollecting astroid<=2.5,>=2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/a8/5133f51967fb21e46ee50831c3f5dda49e976b7f915408d670b1603d41d6/astroid-2.4.2-py3-none-any.whl (213kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 63.4MB/s \n",
            "\u001b[?25hCollecting mccabe<0.7,>=0.6\n",
            "  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: toml>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from pylint->joeynmt==0.0.1) (0.10.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.14->joeynmt==0.0.1) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.14->joeynmt==0.0.1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.14->joeynmt==0.0.1) (3.2.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.14->joeynmt==0.0.1) (1.17.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.14->joeynmt==0.0.1) (1.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (2020.6.20)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->seaborn->joeynmt==0.0.1) (2018.9)\n",
            "Collecting typed-ast<1.5,>=1.4.0; implementation_name == \"cpython\" and python_version < \"3.8\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/ed/5459080d95eb87a02fe860d447197be63b6e2b5e9ff73c2b0a85622994f4/typed_ast-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (737kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 50.5MB/s \n",
            "\u001b[?25hCollecting lazy-object-proxy==1.4.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/dd/b1e3407e9e6913cf178e506cd0dee818e58694d9a5cd1984e3f6a8b9a10f/lazy_object_proxy-1.4.3-cp36-cp36m-manylinux1_x86_64.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=1.14->joeynmt==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=1.14->joeynmt==0.0.1) (1.7.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.14->joeynmt==0.0.1) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.14->joeynmt==0.0.1) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.14->joeynmt==0.0.1) (4.6)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=1.14->joeynmt==0.0.1) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=1.14->joeynmt==0.0.1) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.14->joeynmt==0.0.1) (0.4.8)\n",
            "Building wheels for collected packages: joeynmt, pyyaml, wrapt\n",
            "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for joeynmt: filename=joeynmt-0.0.1-cp36-none-any.whl size=77296 sha256=2b3f7ff0b1fa142c82999766afc5ad0e4f388b14bc658f56e1dd832ddabf714d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-z3qxs6xg/wheels/db/01/db/751cc9f3e7f6faec127c43644ba250a3ea7ad200594aeda70a\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44621 sha256=d19104bc53422a40652eb96d6af3e66f12f7740d6bfe6ea18a8b4dc70eaead8a\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.11.1-cp36-cp36m-linux_x86_64.whl size=67437 sha256=1d564badbc65141c68d757ef480892657b2be222b3fb66b7b39cb0683af2f9e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/67/41/63cbf0f6ac0a6156588b9587be4db5565f8c6d8ccef98202fc\n",
            "Successfully built joeynmt pyyaml wrapt\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: portalocker, sacrebleu, subword-nmt, pyyaml, isort, wrapt, typed-ast, lazy-object-proxy, six, astroid, mccabe, pylint, joeynmt\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: wrapt 1.12.1\n",
            "    Uninstalling wrapt-1.12.1:\n",
            "      Successfully uninstalled wrapt-1.12.1\n",
            "  Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "Successfully installed astroid-2.4.2 isort-5.4.2 joeynmt-0.0.1 lazy-object-proxy-1.4.3 mccabe-0.6.1 portalocker-2.0.0 pylint-2.6.0 pyyaml-5.3.1 sacrebleu-1.4.13 six-1.12.0 subword-nmt-0.3.7 typed-ast-1.4.1 wrapt-1.11.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AaE77Tcppex9"
      },
      "source": [
        "# Preprocessing the Data into Subword BPE Tokens\n",
        "\n",
        "- One of the most powerful improvements for agglutinative languages (a feature of most Bantu languages) is using BPE tokenization [ (Sennrich, 2015) ](https://arxiv.org/abs/1508.07909).\n",
        "\n",
        "- It was also shown that by optimizing the umber of BPE codes we significantly improve results for low-resourced languages [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021) [(Martinus, 2019)](https://arxiv.org/abs/1906.05685)\n",
        "\n",
        "- Below we have the scripts for doing BPE tokenization of our data. We use 4000 tokens as recommended by [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021). You do not need to change anything. Simply running the below will be suitable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H-TyjtmXB1mL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "a22952d4-08d6-4734-e7a0-4f23ec8f40b8"
      },
      "source": [
        "# One of the huge boosts in NMT performance was to use a different method of tokenizing. \n",
        "# Usually, NMT would tokenize by words. However, using a method called BPE gave amazing boosts to performance\n",
        "\n",
        "# Do subword NMT\n",
        "from os import path\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "\n",
        "# Learn BPEs on the training data.\n",
        "os.environ[\"data_path\"] = path.join(\"joeynmt\", \"data\", source_language + target_language) # Herman! \n",
        "! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt -s 4000 -o bpe.codes.4000 --write-vocabulary vocab.$src vocab.$tgt\n",
        "\n",
        "# Apply BPE splits to the development and test data.\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < train.$src > train.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < train.$tgt > train.bpe.$tgt\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < dev.$src > dev.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < dev.$tgt > dev.bpe.$tgt\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test.$src > test.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test.$tgt > test.bpe.$tgt\n",
        "\n",
        "# Create directory, move everyone we care about to the correct location\n",
        "! mkdir -p $data_path\n",
        "! cp train.* $data_path\n",
        "! cp test.* $data_path\n",
        "! cp dev.* $data_path\n",
        "! cp bpe.codes.4000 $data_path\n",
        "! ls $data_path\n",
        "\n",
        "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
        "! cp train.* \"$gdrive_path\"\n",
        "! cp test.* \"$gdrive_path\"\n",
        "! cp dev.* \"$gdrive_path\"\n",
        "! cp bpe.codes.4000 \"$gdrive_path\"\n",
        "! ls \"$gdrive_path\"\n",
        "\n",
        "# Create that vocab using build_vocab\n",
        "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
        "! joeynmt/scripts/build_vocab.py joeynmt/data/$src$tgt/train.bpe.$src joeynmt/data/$src$tgt/train.bpe.$tgt --output_path joeynmt/data/$src$tgt/vocab.txt\n",
        "\n",
        "# Some output\n",
        "! echo \"BPE Tshiluba Sentences\"\n",
        "! tail -n 5 test.bpe.$tgt\n",
        "! echo \"Combined BPE Vocab\"\n",
        "! tail -n 10 joeynmt/data/$src$tgt/vocab.txt  # Herman"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bpe.codes.4000\tdev.en\t     test.bpe.kwy    test.en-any.en.1  train.bpe.kwy\n",
            "dev.bpe.en\tdev.kwy      test.en\t     test.kwy\t       train.en\n",
            "dev.bpe.kwy\ttest.bpe.en  test.en-any.en  train.bpe.en      train.kwy\n",
            "bpe.codes.4000\tdev.en\t     test.bpe.kwy    test.en-any.en.1  train.bpe.kwy\n",
            "dev.bpe.en\tdev.kwy      test.en\t     test.kwy\t       train.en\n",
            "dev.bpe.kwy\ttest.bpe.en  test.en-any.en  train.bpe.en      train.kwy\n",
            "BPE Tshiluba Sentences\n",
            "Ng@@ ub@@ u anene ya lukwikilu ( Tala e tini kia 12 - 14 )\n",
            "E mp@@ u a mv@@ it@@ a a luv@@ ul@@ uku ( Tala e tini kia 15 - 18 )\n",
            "O wantu bet@@ oma yangal@@ alanga vava bemonanga e kiese tukalanga kiau kia sadila Nkand’a Nzambi ye ngolo tuv@@ anganga muna kubasadisa . ”\n",
            "Ns@@ os@@ olo a mwanda ( Tala e tini kia 19 - 20 )\n",
            "T@@ ut@@ omene zaya e mbeni eto , nt@@ ambu miandi ye makani mandi .\n",
            "Combined BPE Vocab\n",
            "Hebre@@\n",
            "‛\n",
            ";@@\n",
            "C.@@\n",
            "č@@\n",
            "ā@@\n",
            "Ł@@\n",
            "−\n",
            "Ṭ@@\n",
            "û\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IlMitUHR8Qy-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "c7fe7702-4494-4a46-bca6-c989c06594c2"
      },
      "source": [
        "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
        "! cp train.* \"$gdrive_path\"\n",
        "! cp test.* \"$gdrive_path\"\n",
        "! cp dev.* \"$gdrive_path\"\n",
        "! cp bpe.codes.4000 \"$gdrive_path\"\n",
        "! ls \"$gdrive_path\""
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bpe.codes.4000\tdev.en\t     test.bpe.kwy    test.en-any.en.1  train.bpe.kwy\n",
            "dev.bpe.en\tdev.kwy      test.en\t     test.kwy\t       train.en\n",
            "dev.bpe.kwy\ttest.bpe.en  test.en-any.en  train.bpe.en      train.kwy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ixmzi60WsUZ8"
      },
      "source": [
        "# Creating the JoeyNMT Config\n",
        "\n",
        "JoeyNMT requires a yaml config. We provide a template below. We've also set a number of defaults with it, that you may play with!\n",
        "\n",
        "- We used Transformer architecture \n",
        "- We set our dropout to reasonably high: 0.3 (recommended in  [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021))\n",
        "\n",
        "Things worth playing with:\n",
        "- The batch size (also recommended to change for low-resourced languages)\n",
        "- The number of epochs (we've set it at 30 just so it runs in about an hour, for testing purposes)\n",
        "- The decoder options (beam_size, alpha)\n",
        "- Evaluation metrics (BLEU versus Crhf4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PIs1lY2hxMsl",
        "colab": {}
      },
      "source": [
        "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
        "# (You can of course play with all the parameters if you'd like!)\n",
        "\n",
        "name = '%s%s' % (source_language, target_language)\n",
        "gdrive_path = os.environ[\"gdrive_path\"]\n",
        "\n",
        "# Create the config\n",
        "config = \"\"\"\n",
        "name: \"{name}_transformer\"\n",
        "\n",
        "data:\n",
        "    src: \"{source_language}\"\n",
        "    trg: \"{target_language}\"\n",
        "    train: \"data/{name}/train.bpe\"\n",
        "    dev:   \"data/{name}/dev.bpe\"\n",
        "    test:  \"data/{name}/test.bpe\"\n",
        "    level: \"bpe\"\n",
        "    lowercase: False\n",
        "    max_sent_length: 100\n",
        "    src_vocab: \"data/{name}/vocab.txt\"\n",
        "    trg_vocab: \"data/{name}/vocab.txt\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "\n",
        "training:\n",
        "    #load_model: \"{gdrive_path}/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
        "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0003\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 4096\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 3600\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 30                     # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "    validation_freq: 1000          # TODO: Set to at least once per epoch.\n",
        "    logging_freq: 100\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"models/{name}_transformer\"\n",
        "    overwrite: False               # TODO: Set to True if you want to overwrite possibly existing models. \n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_last_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "\"\"\".format(name=name, gdrive_path=os.environ[\"gdrive_path\"], source_language=source_language, target_language=target_language)\n",
        "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
        "    f.write(config)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pIifxE3Qzuvs"
      },
      "source": [
        "# Train the Model\n",
        "\n",
        "This single line of joeynmt runs the training using the config we made above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6ZBPFwT94WpI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "02347635-86f5-4ab8-a9d1-e9141399e8d5"
      },
      "source": [
        "# Train the model\n",
        "# You can press Ctrl-C to stop. And then run the next cell to save your checkpoints! \n",
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt.yaml"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/content/joeynmt/joeynmt/__main__.py\", line 41, in <module>\n",
            "    main()\n",
            "  File \"/content/joeynmt/joeynmt/__main__.py\", line 29, in main\n",
            "    train(cfg_file=args.config_path)\n",
            "  File \"/content/joeynmt/joeynmt/training.py\", line 633, in train\n",
            "    trainer = TrainManager(model=model, config=cfg)\n",
            "  File \"/content/joeynmt/joeynmt/training.py\", line 54, in __init__\n",
            "    \"overwrite\", False))\n",
            "  File \"/content/joeynmt/joeynmt/helpers.py\", line 42, in make_model_dir\n",
            "    \"Model directory exists and overwriting is disabled.\")\n",
            "FileExistsError: Model directory exists and overwriting is disabled.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MBoDS09JM807",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "933757b1-490d-4b2f-a9ef-a3058b0f4f13"
      },
      "source": [
        "# Copy the created models from the notebook storage to google drive for persistant storage \n",
        "!cp -r joeynmt/models/${src}${tgt}_transformer/* \"$gdrive_path/models/${src}${tgt}_transformer/\""
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: target '/content/drive/My Drive/masakhane/en-kwy-baseline/models/enkwy_transformer/' is not a directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AYCOImAVVCFJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ba9ae265-056c-4483-b416-71ead95fb4f6"
      },
      "source": [
        "# Copy the created models from the notebook storage to google drive for persistant storage \n",
        "!cp joeynmt/models/${src}${tgt}_transformer/best.ckpt \"$gdrive_path/models/${src}${tgt}_transformer/\""
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot create regular file '/content/drive/My Drive/masakhane/en-kwy-baseline/models/enkwy_transformer/': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cSoGZWeeUFob",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "1a55b9ce-5a06-41ef-f254-879dbd2d1739"
      },
      "source": [
        "!ls joeynmt/models/${src}${tgt}_transformer"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000.hyps  20000.hyps\t30000.hyps  40000.hyps\t50000.hyps  6000.hyps\n",
            "1000.hyps   2000.hyps\t3000.hyps   4000.hyps\t5000.hyps   7000.hyps\n",
            "11000.hyps  21000.hyps\t31000.hyps  41000.hyps\t51000.hyps  8000.hyps\n",
            "12000.hyps  22000.hyps\t32000.hyps  42000.hyps\t52000.hyps  9000.hyps\n",
            "13000.hyps  23000.hyps\t33000.hyps  43000.hyps\t53000.ckpt  best.ckpt\n",
            "14000.hyps  24000.hyps\t34000.hyps  44000.hyps\t53000.hyps  config.yaml\n",
            "15000.hyps  25000.hyps\t35000.hyps  45000.hyps\t54000.hyps  src_vocab.txt\n",
            "16000.hyps  26000.hyps\t36000.hyps  46000.hyps\t55000.ckpt  tensorboard\n",
            "17000.hyps  27000.hyps\t37000.hyps  47000.hyps\t55000.hyps  train.log\n",
            "18000.hyps  28000.hyps\t38000.hyps  48000.hyps\t56000.ckpt  trg_vocab.txt\n",
            "19000.hyps  29000.hyps\t39000.hyps  49000.hyps\t56000.hyps  validations.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n94wlrCjVc17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "44ac49c0-d8b6-490f-b62b-ebf86ceaa043"
      },
      "source": [
        "# Output our validation accuracy\n",
        "! cat \"$gdrive_path/models/${src}${tgt}_transformer/validations.txt\""
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cat: '/content/drive/My Drive/masakhane/en-kwy-baseline/models/enkwy_transformer/validations.txt': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "66WhRE9lIhoD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "00203d12-e68b-4372-b1db-6e4c764d978b"
      },
      "source": [
        "# Test our model\n",
        "! cd joeynmt; python3 -m joeynmt test \"$gdrive_path/models/${src}${tgt}_transformer/config.yaml\""
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-22 01:50:28,730 Hello! This is Joey-NMT.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/content/joeynmt/joeynmt/__main__.py\", line 41, in <module>\n",
            "    main()\n",
            "  File \"/content/joeynmt/joeynmt/__main__.py\", line 32, in main\n",
            "    output_path=args.output_path, save_attention=args.save_attention)\n",
            "  File \"/content/joeynmt/joeynmt/prediction.py\", line 189, in test\n",
            "    cfg = load_config(cfg_file)\n",
            "  File \"/content/joeynmt/joeynmt/helpers.py\", line 164, in load_config\n",
            "    with open(path, 'r') as ymlfile:\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/My Drive/masakhane/en-kwy-baseline/models/enkwy_transformer/config.yaml'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onL3dEwCDyzl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 26,
      "outputs": []
    }
  ]
}